{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076aaa66-5842-43f8-9859-669671fb1f34",
   "metadata": {},
   "source": [
    "## Voting ensembles\n",
    "\n",
    "Voting ensembles ***combine diverse machine learning models using techniques like majority voting or average predictions***. The individual models used in the ensemble could be regression or classification-based algorithms. Once the individual models have been trained, the ensemble can be constructed in a couple of different ways.\n",
    "\n",
    "- ***In regression***, ensembles are created by ***averaging the (weighted or unweighted) predictions***.\n",
    "- ***In classification***, ensembles are created through majority voting (also called hard voting) of predicted classes or through ***(weighted or unweighted) average predicted probabilities (also called soft voting)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb39da-9c8e-43c9-a4b4-80e75c98d649",
   "metadata": {},
   "source": [
    "## Bagging Ensembles\n",
    "\n",
    "Bagging ensembles are created from several instances of a single algorithm that are each trained on different subsets of the training data.\n",
    "\n",
    "There are a few methods that fall under the category of bagging ensembles, and they differ only in the way that the subsets are chosen:\n",
    "\n",
    "- ***Observations with replacement***, the method is called bagging (short for ***bootstrap aggregation***).\n",
    "- ***Observations without replacemen***t, the method is called pasting (not short for anything).\n",
    "- A ***set of features only***, then the method is called ***random subspaces***.\n",
    "- ***Both the features and observations***, then the method is called ***random patches***.\n",
    "\n",
    "Common examples: \n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cdbb61-db6d-4378-a20b-55b4b1eac564",
   "metadata": {},
   "source": [
    "## Boosting Ensembles\n",
    "\n",
    "Boosting ensembles are formed by ***incrementally combining a set of weak learners into a single strong learner***. The term ‘weak learner’ here represents a model that may only be slightly better than taking a random guess at the prediction. In the case of regression, this guess is made by simply using the mean value of the target variable.\n",
    "\n",
    "In contrast to bagging, each ***weak learner is trained on the entire dataset, and no sampling takes place***. However, in boosting, ***each observation in the training data is assigned a weight based on the ability of the algorithm to predict it accurately***.\n",
    "\n",
    "Since ensembles are constructed sequentially, boosting can be very slow and computationally expensive.\n",
    "\n",
    "Popular examples:\n",
    "- ***CatBoost***. Categorical Boosting\n",
    "- ***XGBoost***. Extreme Gradient Boosting \n",
    "- ***LightGBM***. Light Gradient Boosting Machine\n",
    "- AdaBoost (AdaBoostClassifier and AdaBoostRegressor from Sklearn)\n",
    "- (Standard) Gradient boosting (GradientBostingClassifier and GradientBoostingRegressor from Sklearn)\n",
    "- Histogram-based gradient boosting (HistGradientBoostingClassifier and HistGradientBoostingRegressor from Sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad71ce6-48a0-4d6f-a81e-cff18c884fb3",
   "metadata": {},
   "source": [
    "## Stacking Ensembles\n",
    "\n",
    "Stacking is a machine learning strategy that ***combines the predictions of numerous base models, also known as first-level models or base learners, to obtain a final prediction***. It entails training numerous base models on the same training dataset, then feeding their predictions into a higher-level model, also known as a meta-model or second-level model, to make the final prediction. The main idea behind stacking is to ***.combine the predictions of different base models in order to get more extraordinary predictive performance than utilizing a single model.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf298e53-64ea-43d3-b8ed-c2507ac68f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
